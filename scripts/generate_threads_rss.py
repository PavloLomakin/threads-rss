import os
import requests
from bs4 import BeautifulSoup
from feedgen.feed import FeedGenerator
from datetime import datetime, timezone

# ------------ –ù–ê–°–¢–†–û–ô–ö–ò ------------
THREADS_USERNAME = "pavlo.lomakin"
BASE_URL = f"https://www.threads.net/@{THREADS_USERNAME}"
MAX_ITEMS = 20
OUTPUT_PATH = "docs/index.xml"
# -----------------------------------

def fetch_threads_profile_html():
    print(f"üîó –ó–∞–≥—Ä—É–∂–∞—é –ø—Ä–æ—Ñ–∏–ª—å: {BASE_URL}")
    headers = {
        "User-Agent": (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/120.0.0.0 Safari/537.36"
        ),
    }
    resp = requests.get(BASE_URL, headers=headers, timeout=20)
    print(f"‚úÖ –û—Ç–≤–µ—Ç Threads: {resp.status_code}")
    resp.raise_for_status()
    return resp.text


def parse_posts_from_html(html):
    soup = BeautifulSoup(html, "html.parser")
    posts = []

    # Threads –º–µ–Ω—è–µ—Ç –∫–ª–∞—Å—Å—ã ‚Üí —Å–æ–±–∏—Ä–∞–µ–º –í–°–ï <span> —Å —Ç–µ–∫—Å—Ç–æ–º
    candidate_spans = soup.find_all("span")
    text_chunks = []

    for span in candidate_spans:
        text = span.get_text(strip=True)
        if text and len(text) > 20:  # —Ñ–∏–ª—å—Ç—Ä –æ—Ç –º—É—Å–æ—Ä–∞
            text_chunks.append(text)

    print(f"üîç –ù–∞–π–¥–µ–Ω–æ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –±–ª–æ–∫–æ–≤: {len(text_chunks)}")

    if not text_chunks:
        print("‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–∏ –æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –±–ª–æ–∫–∞")
        return []

    # –°–∫–ª–µ–∏–≤–∞–µ–º –≤ –æ–¥–∏–Ω –±–æ–ª—å—à–æ–π –ø–æ—Å—Ç (RSS –≤—Å—ë —Ä–∞–≤–Ω–æ —á–∏—Ç–∞–µ—Ç –∫–∞–∫ –ª–µ–Ω—Ç—É)
    full_text = "\n".join(text_chunks[:MAX_ITEMS])

    posts.append({
        "title": full_text[:80] + ("..." if len(full_text) > 80 else ""),
        "description": full_text,
        "link": BASE_URL,
        "pub_date": datetime.now(timezone.utc),
    })

    print(f"üìä –ò—Ç–æ–≥: –ø–æ—Å—Ç–æ–≤ –¥–ª—è RSS: {len(posts)}")
    return posts


def generate_rss(posts):
    fg = FeedGenerator()
    fg.id(BASE_URL)
    fg.title(f"Threads @{THREADS_USERNAME}")
    fg.link(href=BASE_URL, rel="alternate")
    fg.description(f"RSS –ª–µ–Ω—Ç–∞ –ø–æ—Å—Ç–æ–≤ Threads @{THREADS_USERNAME}")

    for post in posts:
        fe = fg.add_entry()
        fe.id(post["link"] + "#" + post["pub_date"].isoformat())
        fe.title(post["title"])
        fe.link(href=post["link"])
        fe.description(post["description"])
        fe.pubDate(post["pub_date"])

    rss_str = fg.rss_str(pretty=True)

    os.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)
    with open(OUTPUT_PATH, "wb") as f:
        f.write(rss_str)

    print(f"üíæ RSS —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ {OUTPUT_PATH}")


def main():
    try:
        html = fetch_threads_profile_html()
        print("‚úÖ HTML –ø–æ–ª—É—á–µ–Ω")

        posts = parse_posts_from_html(html)

        # –î–∞–∂–µ –µ—Å–ª–∏ –ø–æ—Å—Ç–æ–≤ –Ω–µ—Ç ‚Äî —Å–æ–∑–¥–∞—ë–º –ø—É—Å—Ç–æ–π RSS, —á—Ç–æ–±—ã GitHub Pages –Ω–µ –ª–æ–º–∞–ª—Å—è
        if not posts:
            print("‚ö†Ô∏è –ü–æ—Å—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã ‚Äî —Å–æ–∑–¥–∞—é –ø—É—Å—Ç–æ–π RSS")
            posts = [{
                "title": "No posts found",
                "description": "Threads did not return any readable content.",
                "link": BASE_URL,
                "pub_date": datetime.now(timezone.utc),
            }]

        generate_rss(posts)
        print(f"üéâ –ì–æ—Ç–æ–≤–æ: RSS —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω (–ø–æ—Å—Ç–æ–≤: {len(posts)})")

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≤ main(): {e}")


if __name__ == "__main__":
    main()
