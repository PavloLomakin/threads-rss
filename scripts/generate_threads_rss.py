import os
import requests
from bs4 import BeautifulSoup
from feedgen.feed import FeedGenerator
from datetime import datetime, timezone

# ------------ –ù–ê–°–¢–†–û–ô–ö–ò ------------
THREADS_USERNAME = "pavlo.lomakin"
BASE_URL = f"https://www.threads.net/@{THREADS_USERNAME}"
MAX_ITEMS = 20
OUTPUT_PATH = "docs/index.xml"
# -----------------------------------

def fetch_threads_profile_html():
    print(f"üîó –ó–∞–≥—Ä—É–∂–∞—é –ø—Ä–æ—Ñ–∏–ª—å: {BASE_URL}")
    headers = {
        "User-Agent": (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/120.0.0.0 Safari/537.36"
        ),
    }
    resp = requests.get(BASE_URL, headers=headers, timeout=20)
    print(f"‚úÖ –û—Ç–≤–µ—Ç Threads: {resp.status_code}")
    resp.raise_for_status()
    return resp.text


import json

def parse_posts_from_html(html):
    soup = BeautifulSoup(html, "html.parser")

    # 1. Find the embedded JSON
    script_tag = soup.find("script", id="__NEXT_DATA__")
    if not script_tag:
        print("‚ùå JSON script tag not found")
        return []

    try:
        data = json.loads(script_tag.string)
    except Exception as e:
        print(f"‚ùå Failed to parse JSON: {e}")
        return []

    # 2. Navigate to posts inside the JSON
    try:
        posts_data = (
            data["props"]["pageProps"]["userProfile"]["posts"]
        )
    except KeyError:
        print("‚ùå Posts not found in JSON structure")
        return []

    posts = []
    for post in posts_data[:MAX_ITEMS]:
        text = post.get("caption", "")
        post_id = post.get("id")
        timestamp = post.get("taken_at")

        if not text:
            continue

        # Convert timestamp ‚Üí datetime
        pub_date = datetime.fromtimestamp(timestamp, tz=timezone.utc)

        posts.append({
            "title": text[:80] + ("..." if len(text) > 80 else ""),
            "description": text,
            "link": f"{BASE_URL}/post/{post_id}",
            "pub_date": pub_date,
        })

    print(f"üìä Parsed posts from JSON: {len(posts)}")
    return posts



def generate_rss(posts):
    fg = FeedGenerator()
    fg.id(BASE_URL)
    fg.title(f"Threads @{THREADS_USERNAME}")
    fg.link(href=BASE_URL, rel="alternate")
    fg.description(f"RSS –ª–µ–Ω—Ç–∞ –ø–æ—Å—Ç–æ–≤ Threads @{THREADS_USERNAME}")

    for post in posts:
        fe = fg.add_entry()
        fe.id(post["link"] + "#" + post["pub_date"].isoformat())
        fe.title(post["title"])
        fe.link(href=post["link"])
        fe.description(post["description"])
        fe.pubDate(post["pub_date"])

    rss_str = fg.rss_str(pretty=True)

    os.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)
    with open(OUTPUT_PATH, "wb") as f:
        f.write(rss_str)

    print(f"üíæ RSS —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ {OUTPUT_PATH}")


def main():
    try:
        html = fetch_threads_profile_html()
        print("‚úÖ HTML –ø–æ–ª—É—á–µ–Ω")

        posts = parse_posts_from_html(html)

        # –î–∞–∂–µ –µ—Å–ª–∏ –ø–æ—Å—Ç–æ–≤ –Ω–µ—Ç ‚Äî —Å–æ–∑–¥–∞—ë–º –ø—É—Å—Ç–æ–π RSS, —á—Ç–æ–±—ã GitHub Pages –Ω–µ –ª–æ–º–∞–ª—Å—è
        if not posts:
            print("‚ö†Ô∏è –ü–æ—Å—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã ‚Äî —Å–æ–∑–¥–∞—é –ø—É—Å—Ç–æ–π RSS")
            posts = [{
                "title": "No posts found",
                "description": "Threads did not return any readable content.",
                "link": BASE_URL,
                "pub_date": datetime.now(timezone.utc),
            }]

        generate_rss(posts)
        print(f"üéâ –ì–æ—Ç–æ–≤–æ: RSS —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω (–ø–æ—Å—Ç–æ–≤: {len(posts)})")

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≤ main(): {e}")


if __name__ == "__main__":
    main()
